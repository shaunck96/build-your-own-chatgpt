# -*- coding: utf-8 -*-
"""TextGenEd - Build your own ChatGPT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R8Ix9gBgBTpH_ZtRkw7kqQaryWNNe8M_

##Training a language model using Transformers

Training a language model is a more advanced form of machine learning, therefore we need to install some libraries (pre-written pieces of code) that are not default for Google Colab. We also clone our GitHub repository as usual.

To do these steps, hove your mouse over the mouse over the empty [ ] below and click the play button.
"""

# prompt: pip install transformers

!pip install transformers

"""# New Section

After installing the libraries, we need to import them so we can use them in our code. This makes training a language model much less code intensive
"""

!pip install datasets
!pip install transformers
!pip install sentencepiece
!pip install accelerate
!git clone https://github.com/Joaoffg/AISocIMP23/

from datasets import load_dataset
import re
import os
import torch
from transformers import LlamaTokenizer, LlamaConfig, LlamaModel, LlamaForCausalLM, Trainer, TrainingArguments

!wget https://surfdrive.surf.nl/files/index.php/s/1WPoowu5oaFAvWu/download
!unzip download

"""This part loads all of the text data that you will use to train the language model. If you click on the folder icon to the left side of the screen, you can see exactly what data is being used. Feel free to add your own txt data files, or delete some of the ones that are there."""

from google.colab import drive
drive.mount('/content/drive')

!pip install chardet
import chardet

with open("/content/LLM_workshop_data/ThesisDraft_MariaPalaciosBarea_622509.txt", 'rb') as rawdata:
    result = chardet.detect(rawdata.read(10000))
encoding = result['encoding']
print(f"Detected encoding: {encoding}")
dataset = load_dataset("text",
                       data_dir="/content/LLM_workshop_data",
                       encoding=encoding)

"""This part tokenizes the dataset, which means that it converts all of the words into numbers that can be processed by the neural network."""

tokenizer = LlamaTokenizer.from_pretrained('/content/AISocIMP23/Week 4/Token')

def chunk_examples(examples,chunk_lenght=128, min_chunk_lenght = 25):
    chunks = []
    for text in examples["text"]:
        tokenized = tokenizer(text,add_special_tokens=False)
        if len(tokenized.input_ids) > min_chunk_lenght:
            input_ids = [tokenizer.bos_token_id] + tokenized.input_ids + [tokenizer.eos_token_id]
            attention_mask = [1] + tokenized.attention_mask + [1]
            for i in range(0, len(tokenized.input_ids), chunk_lenght):
                cunk_input_ids = input_ids[i:i + chunk_lenght]
                cunk_att_mask = attention_mask[i:i + chunk_lenght]
                cur_chunk_len = len(cunk_input_ids)

                if  cur_chunk_len < chunk_lenght:
                    cunk_input_ids = cunk_input_ids + [tokenizer.eos_token_id]*(chunk_lenght - cur_chunk_len)
                    cunk_att_mask = cunk_att_mask + [0]*(chunk_lenght - cur_chunk_len)

                chunks += [{"input_ids":torch.tensor(cunk_input_ids),
                        "attention_mask": torch.tensor(cunk_att_mask),
                        "labels": torch.tensor(cunk_input_ids)}
                        #"raw":tokenizer.decode(cunk_input_ids)}

                        ]


    return {"chunks": chunks}


chunked_dataset = dataset.map(chunk_examples, batched=True, remove_columns=['text'])

"""Here we define our model architecture, we are using a LlaMa based model for this exercise. You can change the complexity factor below to make the model more simple or more complex."""

complexity_reduction=4

config = LlamaConfig(
    vocab_size = 32000,
    hidden_size= int(2048/complexity_reduction),
    intermediate_size = int(5120/complexity_reduction),
    num_hidden_layers = int(16/complexity_reduction),
    num_attention_heads = int(16/complexity_reduction),
    max_position_embeddings = 2048 ,
    rms_norm_eps = 1e-12
)

model = LlamaForCausalLM(config)
model_size = sum(t.numel() for t in model.parameters())
print(f"LlaMa Model Size: {model_size/1000**2:.1f}M parameters")

tokenizer.pad_token = tokenizer.eos_token

"""Here you define the training arguments. You can ignore most of them, they are default values, but you may want to tweak the batch_sizes and the learning rate to decide how quickly the argument learns. Higher values mean faster learning, but the end result may not be as good."""

args = TrainingArguments(
    output_dir="erasmian-lm/medium",
    report_to=[],
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    evaluation_strategy="no",
    eval_steps=5_000,
    logging_steps=5_000,
    num_train_epochs=2,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type="cosine",
    learning_rate=0.0001,
    save_steps=5_000,
    fp16=True,
    save_strategy = "epoch", #save only latest model at end of epoch instead of 5k steps.
    save_total_limit = 1 # save only latest 3 epochs
)

"""This step trains the model, exciting! It will now go over the training data you provided to learn patterns in language and replicate them.

NOTE: This step takes longer than the others, depending on how many texts you are training your model on. If you feel like it is taking too long, restart whole the process from the beguinning and delete some additional texts.
"""

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    train_dataset=chunked_dataset['train']['chunks']

)

model = torch.compile(model)
trainer.train()

"""And here you can test how the model is actually performing by generating some text. You can write the start of the text between "  " in the input_text field.

Experiment with some alternatives, such as:
- "A language model is "
- "Summer is "
- "I feel "
- "Yesterday, I had "
"""

from transformers import GenerationConfig

input_text= "I want to"

generation_config = GenerationConfig(
    temperature=0.7,
    top_k=50,
    max_new_tokens=100
)

inputs = tokenizer(input_text, return_tensors="pt")
inputs = inputs.to("cuda:0")
model = model.to("cuda:0")
outputs = model.generate(**inputs, num_beams=1, generation_config=generation_config)
print(tokenizer.batch_decode(outputs, skip_special_tokens=True))

"""This final step saves the model so that you can download it and reuse it whenever you like.

> Add blockquote


"""

import locale
locale.getpreferredencoding = lambda: "UTF-8"

model.save_pretrained("my_own_GPT", from_pt=True)
!zip -r my_own_gpt.zip my_own_GPT